{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5JT55fIdpKc"
      },
      "outputs": [],
      "source": [
        "#Declaring the hyperparameter\n",
        "numDimensions = 300\n",
        "maxSeqLength = 250\n",
        "batchSize = 24\n",
        "lstmUnits = 64\n",
        "numClasses = 2\n",
        "iterations = 100000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data Structures\n",
        "import numpy as np\n",
        "wordsList = np.load('wordsList.npy').tolist()\n",
        "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
        "wordVectors = np.load('wordVectors.npy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7ynviNMIdxll",
        "outputId": "cf3de002-d2a8-4875-83aa-35970d4d401a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cfba6d9d105b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Loading Data Structures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwordsList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordsList.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwordsList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Encode words as UTF-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwordVectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordVectors.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wordsList.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the graph\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
        "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
        "\n",
        "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
        "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
        "\n",
        "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
        "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.25)\n",
        "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
        "\n",
        "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
        "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
        "value = tf.transpose(value, [1, 0, 2])\n",
        "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
        "prediction = (tf.matmul(last, weight) + bias)\n",
        "\n",
        "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
      ],
      "metadata": {
        "id": "lwtSeWYWd2W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the network\n",
        "sess = tf.InteractiveSession()\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, tf.train.latest_checkpoint('models'))"
      ],
      "metadata": {
        "id": "shBpHghJeKVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
        "import re\n",
        "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
        "\n",
        "def cleanSentences(string):\n",
        "    string = string.lower().replace(\"<br />\", \" \")\n",
        "    return re.sub(strip_special_chars, \"\", string.lower())\n",
        "\n",
        "def getSentenceMatrix(sentence):\n",
        "    arr = np.zeros([batchSize, maxSeqLength])\n",
        "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
        "    cleanedSentence = cleanSentences(sentence)\n",
        "    split = cleanedSentence.split()\n",
        "    for indexCounter,word in enumerate(split):\n",
        "        try:\n",
        "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
        "        except ValueError:\n",
        "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
        "    return sentenceMatrix"
      ],
      "metadata": {
        "id": "Ay7FnZ5yeQdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputText = input(\"How was your day? \")\n",
        "inputMatrix = getSentenceMatrix(inputText)"
      ],
      "metadata": {
        "id": "Bj7dGjJ-eVTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
        "# predictedSentiment[0] represents output score for positive sentiment\n",
        "# predictedSentiment[1] represents output score for negative sentiment\n",
        "\n",
        "if (predictedSentiment[0] > predictedSentiment[1]):\n",
        "    print \"Positive Sentiment\"\n",
        "else:\n",
        "    print \"Negative Sentiment\""
      ],
      "metadata": {
        "id": "oDEizWy4ekNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fveg0jx0e3qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bq-MseNvIr9t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}